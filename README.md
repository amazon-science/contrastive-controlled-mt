## CoCoA-MT: A Dataset and Benchmark for Contrastive Controlled MT with Application to Formality

This repository contains the CoCoA-MT dataset and benchmark for formality control.
Participants to the [Formality Control for SLT](https://iwslt.org/2022/formality) shared task can find the annotated dataset and evaluation script under [IWSLT2022/](/IWSLT2022/). 


## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This project is licensed under the CDLA-Sharing-1.0 License.

## Citation

If you are a participant in the IWSLT shared task on Formality Control for SLT, or are otherwise using the resources 
from this repository in your work, please cite:
```
@inproceedings{nadejde-etal-2022-coca-mt,
    title = "{C}o{C}o{A}-{MT}: A Dataset and Benchmark for {Co}ntrastive {Co}ntrolled {MT} with Application to Formality",
    author = "N\u{a}dejde, Maria  and Currey, Anna  and Hsu, Benjamin  and Niu, Xing and Federico, Marcello and Dinu, Georgiana",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, USA",
    publisher = "Association for Computational Linguistics",
}
```

If you use the [topical-chat](https://github.com/alexa/Topical-Chat) part of the dataset, in addition to the citation above, please also cite:
```
@inproceedings{Gopalakrishnan2019, 
  author={Karthik Gopalakrishnan and Behnam Hedayatnia and Qinlang Chen and Anna Gottardi and Sanjeev Kwatra and Anu Venkatesh and Raefer Gabriel and Dilek Hakkani-TÃ¼r},
  title={{Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={1891--1895},
  doi={10.21437/Interspeech.2019-3079},
  url={http://dx.doi.org/10.21437/Interspeech.2019-3079}
}
```
